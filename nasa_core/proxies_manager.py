from utils.logger import get_logger
from nasa_core.base_spider import BaseSpider
from config.settings import BASE_DIR, CONFIG_DIR, PROXY_POOL_CONFIG
import requests
import threading
import json
import os
from datetime import datetime
import queue
import importlib
import yaml
from typing import Dict

logger = get_logger()


class ProxiesManager():
    """ çˆ¬è™«ç®¡ç†"""

    _instance = None
    _initialized = False
    _lock = threading.Lock()
    # è‡ªåŠ¨åˆå§‹åŒ–æ—¶åŠ è½½ç¼“å­˜

    def __new__(cls, *args, **kwargs):
        with cls._lock:
            if not cls._instance:
                cls._instance = super().__new__(cls)
            return cls._instance

    def __init__(self):
        if not self._initialized:
            self.headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
                "(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            }
            self.proxy_spiders: Dict[str, BaseSpider] = {}
            self.proxy_spiders_dir = os.path.join(BASE_DIR, "spiders", "proxy")
            self.proxy_pool = []
            self.proxy_pool_queue = queue.Queue()
            self._crawl_executed = False
            self._initialized = True
            self.cache_file = PROXY_POOL_CONFIG.get("cache_file", os.path.join(BASE_DIR, "cache", "proxies_cache.json"))
            self._load_proxy_spiders()

    def _load_proxy_spiders(self):
        """åŠ è½½çˆ¬è™«ç±»"""
        with open(os.path.join(CONFIG_DIR, "spiders_config.yaml"), "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        enabled_proxy_spiders = config.get("enabled_proxy_spiders", [])

        spider_display_names = {
            spider["name"]: spider["display_name"]
            for spider in enabled_proxy_spiders
            if "name" in spider and "display_name" in spider
        }

        for filename in os.listdir(self.proxy_spiders_dir):
            if filename.endswith(".py") and filename != "__init__.py":
                try:
                    module_name = filename[:-3]
                    full_path = f"spiders.proxy.{module_name}"

                    # åŠ¨æ€å¯¼å…¥æ¨¡å—
                    module = importlib.import_module(full_path)

                    for name, obj in vars(module).items():
                        if (isinstance(obj, type) and
                            issubclass(obj, BaseSpider) and
                                obj is not BaseSpider):

                            if name not in spider_display_names:
                                continue

                            self.proxy_spiders[spider_display_names[name]] = obj
                            logger.info(f"å·²åŠ è½½çˆ¬è™«ç±»: {name}")
                except Exception as e:
                    logger.error(f"åŠ è½½çˆ¬è™«ç±»å¤±è´¥: {filename} - {str(e)}")

    def _load_cache(self):
        """åŠ è½½æœ¬åœ°ç¼“å­˜æ–‡ä»¶"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r') as f:
                    data = json.load(f)
                    # self.proxy_pool = data['proxies']
                    self.last_update = datetime.fromisoformat(data['last_update'])
                    now = datetime.now()
                    if (now - self.last_update).seconds < PROXY_POOL_CONFIG.get("update_interval", 60):
                        self.proxy_pool = data['proxies']
                        logger.info(f"âœ… å·²åŠ è½½ç¼“å­˜ä»£ç† ({len(self.proxy_pool)}) ä¸Šæ¬¡æ›´æ–°: {self.last_update}")
                    else:
                        logger.info("âš ï¸ ç¼“å­˜ä»£ç†å·²è¿‡æœŸï¼Œé‡æ–°çˆ¬å–...")
                        self._refresh_proxies()
            except Exception as e:
                logger.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé‡å»ºç¼“å­˜: {str(e)}")
                self._refresh_proxies()
        else:
            self._refresh_proxies()
        for proxy in self.proxy_pool:
            self.proxy_pool_queue.put(proxy)

    def init_proxies(self):
        self._load_cache()
        if not self.proxy_pool:
            without_proxy = input("ä»£ç†æ± ä¸ºç©º,æ˜¯å¦ä¸ä½¿ç”¨ä»£ç†ç»§ç»­ï¼Ÿ(y/n)")
            if without_proxy == "n":
                logger.error("ä»£ç†æ± ä¸ºç©ºï¼Œæ— æ³•è·å–ä»£ç†")
                exit(0)

    def fetch(self, url):
        try:
            response = requests.get(url, headers=self.headers)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"è¯·æ±‚ {url} æ—¶å‡ºé”™ï¼š{e}")
            return None

    def _refresh_proxies(self):
        """è‡ªåŠ¨çˆ¬å–æœºåˆ¶"""
        logger.info("ğŸ”„ æ­£åœ¨çˆ¬å–æ–°ä»£ç†...")
        self.crawl()

        if self.proxy_pool:
            logger.info("ğŸ” å¼€å§‹æ£€æŸ¥ä»£ç†å¯ç”¨æ€§...")
            self.check_proxies()
            logger.info(f"ğŸ” å…±çˆ¬å–åˆ° {len(self.proxy_pool)} ä¸ªå¯ç”¨ä»£ç†")
            self.last_update = datetime.now()
            self._save_cache()

    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        data = {
            'last_update': datetime.now().isoformat(),
            'proxies': self.proxy_pool
        }
        with open(self.cache_file, 'w') as f:
            json.dump(data, f, indent=2)
        logger.info(f"ğŸ’¾ ä»£ç†æ± å·²ç¼“å­˜ ({len(self.proxy_pool)}ä¸ªä»£ç†)")

    def get_proxies(self):
        if not self.proxy_pool:
            logger.warning("ä»£ç†æ± ä¸ºç©º")
            return None
        return self.proxy_pool_queue.get()

    def release_proxies(self, proxy):
        self.proxy_pool_queue.put(proxy)

    def get_proxies_count(self):
        return len(self.proxy_pool)

    def crawl(self):
        for name, spider_class in self.proxy_spiders.items():
            try:
                spider = spider_class(self.headers, with_proxy=False)
                html = spider.fetch(spider.url)
                if html:
                    proxies = spider.parse(html)
                    logger.info(f"ğŸ” ä»£ç†ç½‘ç«™ {name} å…±çˆ¬å–åˆ° {len(proxies)} ä¸ªä»£ç†")
                    if proxies:
                        self.proxy_pool.extend(proxies)
            except Exception as e:
                logger.error(f"çˆ¬å–ä»£ç†å¤±è´¥: {name} - {str(e)}")
                continue

    def check_proxies(self):
        threads = []
        for proxy in self.proxy_pool:
            thread = threading.Thread(target=self._check_proxy, args=(proxy,))
            threads.append(thread)
            thread.start()
        for t in threads:
            t.join()

    def _check_proxy(self, proxy):
        try:
            response = requests.get("https://httpbin.org/ip", proxies=proxy, timeout=10)
            response.raise_for_status()
            logger.debug(f"ä»£ç† {proxy} å¯ç”¨ã€‚")
        except Exception as e:
            logger.debug(f"ä»£ç† {proxy} ä¸å¯ç”¨ï¼š{e}")
            self.proxy_pool.remove(proxy)
