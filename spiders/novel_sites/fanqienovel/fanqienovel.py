# spiders/fanqienovel.py
from parsel import Selector
from nasa_core.base_spider import BaseSpider
from utils.logger import get_logger
from nasa_core.proxies_manager import ProxiesManager
from spiders.novel_sites.fanqienovel.settings import FANQIE_URL, BASE_URL, CRAWL_RULE
import random
import time
from concurrent.futures import ThreadPoolExecutor

logger = get_logger()

# å­—ç¬¦æ˜ å°„å­—å…¸ï¼šç”¨äºè§£å¯†é¡µé¢ä¸­ç»è¿‡æ˜ å°„è½¬æ¢çš„å­—ç¬¦
dit_data = {
    '58670': '0',
    '58413': '1',
    '58678': '2',
    '58371': '3',
    '58353': '4',
    '58480': '5',
    '58359': '6',
    '58449': '7',
    '58540': '8',
    '58692': '9',
    '58712': 'a',
    '58542': 'b',
    '58575': 'c',
    '58626': 'd',
    '58691': 'e',
    '58561': 'f',
    '58362': 'g',
    '58619': 'h',
    '58430': 'i',
    '58531': 'j',
    '58588': 'k',
    '58440': 'l',
    '58681': 'm',
    '58631': 'n',
    '58376': 'o',
    '58429': 'p',
    '58555': 'q',
    '58498': 'r',
    '58518': 's',
    '58453': 't',
    '58397': 'u',
    '58356': 'v',
    '58435': 'w',
    '58514': 'x',
    '58482': 'y',
    '58529': 'z',
    '58515': 'A',
    '58688': 'B',
    '58709': 'C',
    '58344': 'D',
    '58656': 'E',
    '58381': 'F',
    '58576': 'G',
    '58516': 'H',
    '58463': 'I',
    '58649': 'J',
    '58571': 'K',
    '58558': 'L',
    '58433': 'M',
    '58517': 'N',
    '58387': 'O',
    '58687': 'P',
    '58537': 'Q',
    '58541': 'R',
    '58458': 'S',
    '58390': 'T',
    '58466': 'U',
    '58386': 'V',
    '58697': 'W',
    '58519': 'X',
    '58511': 'Y',
    '58634': 'Z',
    '58611': 'çš„',
    '58590': 'ä¸€',
    '58398': 'æ˜¯',
    '58422': 'äº†',
    '58657': 'æˆ‘',
    '58666': 'ä¸',
    '58562': 'äºº',
    '58345': 'åœ¨',
    '58510': 'ä»–',
    '58496': 'æœ‰',
    '58654': 'è¿™',
    '58441': 'ä¸ª',
    '58493': 'ä¸Š',
    '58714': 'ä»¬',
    '58618': 'æ¥',
    '58528': 'åˆ°',
    '58403': 'å¤§',
    '58461': 'åœ°',
    '58481': 'ä¸º',
    '58700': 'å­',
    '58708': 'ä¸­',
    '58503': 'ä½ ',
    '58442': 'è¯´',
    '58639': 'ç”Ÿ',
    '58506': 'å›½',
    '58663': 'å¹´',
    '58436': 'ç€',
    '58563': 'å°±',
    '58391': 'é‚£',
    '58357': 'å’Œ',
    '58354': 'è¦',
    '58695': 'å¥¹',
    '58372': 'å‡º',
    '58696': 'ä¹Ÿ',
    '58551': 'å¾—',
    '58445': 'é‡Œ',
    '58408': 'å',
    '58599': 'è‡ª',
    '58424': 'ä»¥',
    '58394': 'ä¼š',
    '58348': 'å®¶',
    '58426': 'å¯',
    '58673': 'ä¸‹',
    '58417': 'è€Œ',
    '58556': 'è¿‡',
    '58603': 'å¤©',
    '58565': 'å»',
    '58604': 'èƒ½',
    '58522': 'å¯¹',
    '58632': 'å°',
    '58622': 'å¤š',
    '58350': 'ç„¶',
    '58605': 'äº',
    '58617': 'å¿ƒ',
    '58401': 'å­¦',
    '58637': 'ä¹ˆ',
    '58684': 'ä¹‹',
    '58382': 'éƒ½',
    '58464': 'å¥½',
    '58487': 'çœ‹',
    '58693': 'èµ·',
    '58608': 'å‘',
    '58392': 'å½“',
    '58474': 'æ²¡',
    '58601': 'æˆ',
    '58355': 'åª',
    '58573': 'å¦‚',
    '58499': 'äº‹',
    '58469': 'æŠŠ',
    '58361': 'è¿˜',
    '58698': 'ç”¨',
    '58489': 'ç¬¬',
    '58711': 'æ ·',
    '58457': 'é“',
    '58635': 'æƒ³',
    '58492': 'ä½œ',
    '58647': 'ç§',
    '58623': 'å¼€',
    '58521': 'ç¾',
    '58609': 'æ€»',
    '58530': 'ä»',
    '58665': 'æ— ',
    '58652': 'æƒ…',
    '58676': 'å·±',
    '58456': 'é¢',
    '58581': 'æœ€',
    '58509': 'å¥³',
    '58488': 'ä½†',
    '58363': 'ç°',
    '58685': 'å‰',
    '58396': 'äº›',
    '58523': 'æ‰€',
    '58471': 'åŒ',
    '58485': 'æ—¥',
    '58613': 'æ‰‹',
    '58533': 'åˆ',
    '58589': 'è¡Œ',
    '58527': 'æ„',
    '58593': 'åŠ¨',
    '58699': 'æ–¹',
    '58707': 'æœŸ',
    '58414': 'å®ƒ',
    '58596': 'å¤´',
    '58570': 'ç»',
    '58660': 'é•¿',
    '58364': 'å„¿',
    '58526': 'å›',
    '58501': 'ä½',
    '58638': 'åˆ†',
    '58404': 'çˆ±',
    '58677': 'è€',
    '58535': 'å› ',
    '58629': 'å¾ˆ',
    '58577': 'ç»˜',
    '58606': 'å¤š',
    '58497': 'æ³•',
    '58662': 'é—´',
    '58479': 'æ–¯',
    '58532': 'çŸ¥',
    '58380': 'ä¸–',
    '58385': 'ä»€',
    '58405': 'ä¸¤',
    '58644': 'æ¬¡',
    '58578': 'ä½¿',
    '58505': 'èº«',
    '58564': 'è€…',
    '58412': 'è¢«',
    '58686': 'é«˜',
    '58624': 'å·²',
    '58667': 'äº²',
    '58607': 'å…¶',
    '58616': 'è¿›',
    '58368': 'æ­¤',
    '58427': 'è¯',
    '58423': 'å¸¸',
    '58633': 'ä¸',
    '58525': 'æ´»',
    '58543': 'æ­£',
    '58418': 'æ„Ÿ',
    '58597': 'è§',
    '58683': 'æ˜',
    '58507': 'é—®',
    '58621': 'åŠ›',
    '58703': 'ç†',
    '58438': 'å°”',
    '58536': 'å ',
    '58384': 'æ–‡',
    '58484': 'å‡ ',
    '58539': 'å®š',
    '58554': 'æœ¨',
    '58421': 'å…¬',
    '58347': 'ç‰¹',
    '58569': 'åš',
    '58710': 'å¤–',
    '58574': 'å­©',
    '58375': 'ç›¸',
    '58645': 'è¥¿',
    '58592': 'æœ',
    '58572': 'èµ°',
    '58388': 'å°†',
    '58370': 'æœˆ',
    '58399': 'å',
    '58651': 'å®',
    '58546': 'å‘',
    '58504': 'å£°',
    '58419': 'è½¦',
    '58407': 'å…¨',
    '58672': 'ä¿¡',
    '58675': 'é‡',
    '58538': 'ä¸‰',
    '58465': 'æœº',
    '58374': 'å·¥',
    '58579': 'ç‰©',
    '58402': 'æ°”',
    '58702': 'æ¯',
    '58553': 'å¹¶',
    '58360': 'åˆ«',
    '58389': 'çœŸ',
    '58560': 'æ‰“',
    '58690': 'å¤ª',
    '58473': 'æ–°',
    '58512': 'æ¯”',
    '58653': 'æ‰',
    '58704': 'ä¾¿',
    '58545': 'å¤«',
    '58641': 'å†',
    '58475': 'ä¹¦',
    '58583': 'éƒ¨',
    '58472': 'æ°´',
    '58478': 'åƒ',
    '58664': 'çœ¼',
    '58586': 'ç­‰',
    '58568': 'ä½“',
    '58674': 'å´',
    '58490': 'åŠ ',
    '58476': 'ç”µ',
    '58346': 'ä¸»',
    '58630': 'ç•Œ',
    '58595': 'é—¨',
    '58502': 'åˆ©',
    '58713': 'æµ·',
    '58587': 'å—',
    '58548': 'å¬',
    '58351': 'è¡¨',
    '58547': 'å¾·',
    '58443': 'å°‘',
    '58460': 'å…‹',
    '58636': 'ä»£',
    '58585': 'å‘˜',
    '58625': 'è®¸',
    '58694': 'ç¨œ',
    '58428': 'å…ˆ',
    '58640': 'å£',
    '58628': 'ç”±',
    '58612': 'æ­»',
    '58446': 'å®‰',
    '58468': 'å†™',
    '58410': 'æ€§',
    '58508': 'é©¬',
    '58594': 'å…‰',
    '58483': 'ç™½',
    '58544': 'æˆ–',
    '58495': 'ä½',
    '58450': 'éš¾',
    '58643': 'æœ›',
    '58486': 'æ•™',
    '58406': 'å‘½',
    '58447': 'èŠ±',
    '58669': 'ç»“',
    '58415': 'ä¹',
    '58444': 'è‰²',
    '58549': 'æ›´',
    '58494': 'æ‹‰',
    '58409': 'ä¸œ',
    '58658': 'ç¥',
    '58557': 'è®°',
    '58602': 'å¤„',
    '58559': 'è®©',
    '58610': 'æ¯',
    '58513': 'çˆ¶',
    '58500': 'åº”',
    '58378': 'ç›´',
    '58680': 'å­—',
    '58352': 'åœº',
    '58383': 'å¹³',
    '58454': 'æŠ¥',
    '58671': 'å‹',
    '58668': 'å…³',
    '58452': 'æ”¾',
    '58627': 'è‡³',
    '58400': 'å¼ ',
    '58455': 'è®¤',
    '58416': 'æ¥',
    '58552': 'å‘Š',
    '58614': 'å…¥',
    '58582': 'ç¬‘',
    '58534': 'å†…',
    '58701': 'è‹±',
    '58349': 'å†›',
    '58491': 'å€™',
    '58467': 'æ°‘',
    '58365': 'å²',
    '58598': 'å¾€',
    '58425': 'ä½•',
    '58462': 'åº¦',
    '58420': 'å±±',
    '58661': 'è§‰',
    '58615': 'è·¯',
    '58648': 'å¸¦',
    '58470': 'ä¸‡',
    '58377': 'ç”·',
    '58520': 'è¾¹',
    '58646': 'é£',
    '58600': 'è§£',
    '58431': 'å«',
    '58715': 'ä»»',
    '58524': 'é‡‘',
    '58439': 'å¿«',
    '58566': 'åŸ',
    '58477': 'åƒ',
    '58642': 'å¦ˆ',
    '58437': 'å˜',
    '58411': 'é€š',
    '58451': 'å¸ˆ',
    '58395': 'ç«‹',
    '58369': 'è±¡',
    '58706': 'æ•°',
    '58705': 'å››',
    '58379': 'å¤±',
    '58567': 'æ»¡',
    '58373': 'æˆ˜',
    '58448': 'è¿œ',
    '58659': 'æ ¼',
    '58434': 'å£«',
    '58679': 'éŸ³',
    '58432': 'è½»',
    '58689': 'ç›®',
    '58591': 'æ¡',
    '58682': 'å‘¢',
}


class FanqieNovelSpider(BaseSpider):
    def __init__(self, headers=None, with_proxy=True):
        super().__init__(headers, with_proxy)
        self.urls = FANQIE_URL

    def crawl(self) -> list:
        decoded_text_list = []
        max_workers = ProxiesManager().get_proxies_count() or 1
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for name, url in self.urls.items():
                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(
                    self.crawl_task,
                    url=url,
                    task_name=name,
                )
                futures.append(future)

            # æ”¶é›†ç»“æœ
            for future in futures:
                result = future.result()
                if result:
                    decoded_text_list.extend(result)
        if decoded_text_list:
            return decoded_text_list
        return None

    def crawl_task(self, url, task_name):
        logger.info(f"ğŸš€ æ­£åœ¨çˆ¬å–æ¦œå•{task_name}")
        html = self.fetch(url)
        if html:
            content_list = self.parse(html)
            # ç•ªèŒ„å°è¯´ç½‘ç«™çš„å†…å®¹ç»è¿‡äº†å­—ç¬¦æ˜ å°„è½¬æ¢ï¼Œéœ€è¦è§£å¯†
            return self.decode_content(content_list)

    def parse(self, html):
        content_list = []
        selector = Selector(html)
        # æ ¹æ®å®é™…é¡µé¢ç»“æ„æå–ç›®æ ‡æ–‡æœ¬
        if CRAWL_RULE == "FULL":
            books_list = selector.css('div.book-item-text > div.title a::attr(href)').getall()
            for book in books_list:
                book_content = ""
                book_url = BASE_URL + book
                book_html = self.fetch(book_url)
                if book_html:
                    book_selector = Selector(book_html)
                    # æå–ä¹¦ç±ä¿¡æ¯
                    name = book_selector.css('div.info-name > h1::text').get()
                    book_content += name
                    labels = book_selector.css('div.info-label > span.info-label-grey::text').getall()
                    book_content += " ".join(labels)
                    segments = book_selector.css('div.page-abstract-content > p::text').getall()
                    book_content += " ".join(segments)
                    content_list.append(book_content)
                time.sleep(random.uniform(1, 2))
        elif CRAWL_RULE == "SIMPLE":
            book_titles = selector.css('div.book-item-text > div.title a::text').getall()
            book_abstracts = selector.css('div.book-item-text > div.desc.abstract.font-DNMrHsV173Pd4pgy::text').getall()
            for title, abstract in zip(book_titles, book_abstracts):
                book_content = title + " " + abstract
                content_list.append(book_content)

        if content_list:
            return content_list
        if not content_list:
            logger.warning("é¡µé¢ä¸­æœªæå–åˆ°å†…å®¹ï¼Œè¯·æ£€æŸ¥é€‰æ‹©å™¨æˆ–é¡µé¢ç»“æ„ã€‚")
        return content_list

    def decode_content(self, content_list):
        """
        å®ç°è¯¥ç½‘ç«™ä¸“å±çš„è§£å¯†é€»è¾‘ï¼š
        ç¤ºä¾‹ä¸­ä½¿ç”¨ MAPPING_DICT è¿›è¡Œå­—ç¬¦æ˜ å°„è½¬æ¢ï¼Œ
        å¦‚æœæŸä¸ªå­—ç¬¦çš„ ord å€¼åœ¨å­—å…¸ä¸­æœ‰å¯¹åº”ï¼Œåˆ™æ›¿æ¢æˆå¯¹åº”å­—ç¬¦ï¼Œå¦åˆ™ä¿æŒåŸæ ·ã€‚
        """
        decoded_text_list = []
        for content in content_list:
            decoded_text = ""
            for char in content:
                decoded_text += dit_data.get(str(ord(char)), char)
            decoded_text_list.append(decoded_text)
        return decoded_text_list
