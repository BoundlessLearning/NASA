from parsel import Selector
from nasa_core.base_spider import BaseSpider
from utils.logger import get_logger
from nasa_core.proxies_manager import ProxiesManager
from spiders.novel_sites.qidiannovel.settings import QIDIAN_URL, CRAWL_PAGE
from concurrent.futures import ThreadPoolExecutor

logger = get_logger()


class QidianNovelSpider(BaseSpider):
    def __init__(self, headers=None, with_proxy=False):
        super().__init__(headers, with_proxy)
        self.urls = {}
        for i in range(1, CRAWL_PAGE + 1):
            self.urls[f"ç¬¬{i}é¡µ"] = f"{QIDIAN_URL}page{i}/"

    def crawl(self) -> list:
        decoded_text_list = []
        max_workers = ProxiesManager().get_proxies_count() or 1
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            for name, url in self.urls.items():
                # æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
                future = executor.submit(
                    self.crawl_task,
                    url=url,
                    task_name=name,
                )
                futures.append(future)

            # æ”¶é›†ç»“æœ
            for future in futures:
                result = future.result()
                if result:
                    decoded_text_list.extend(result)
        if decoded_text_list:
            return decoded_text_list
        return None

    def crawl_task(self, url, task_name):
        logger.info(f"ğŸš€ æ­£åœ¨çˆ¬å–æ¦œå•{task_name}")
        html = self.selenium_fetch(url)
        if html:
            content_list = self.parse(html)
            # ç•ªèŒ„å°è¯´ç½‘ç«™çš„å†…å®¹ç»è¿‡äº†å­—ç¬¦æ˜ å°„è½¬æ¢ï¼Œéœ€è¦è§£å¯†
            return content_list
        return None

    def parse(self, html):
        content_list = []
        selector = Selector(html)
        # æ ¹æ®å®é™…é¡µé¢ç»“æ„æå–ç›®æ ‡æ–‡æœ¬

        book_titles = selector.css('div.book-mid-info h2 a::text').getall()
        book_abstracts = selector.css('div.book-mid-info p.intro::text').getall()
        for title, abstract in zip(book_titles, book_abstracts):
            book_content = title + " " + abstract
            content_list.append(book_content)

        if content_list:
            return content_list
        if not content_list:
            logger.warning("é¡µé¢ä¸­æœªæå–åˆ°å†…å®¹ï¼Œè¯·æ£€æŸ¥é€‰æ‹©å™¨æˆ–é¡µé¢ç»“æ„ã€‚")
        return content_list


if __name__ == "__main__":
    spider = QidianNovelSpider()
    result = spider.crawl()
    if result:
        for i, content in enumerate(result):
            print(f"å†…å®¹ {i + 1}: {content}")
    else:
        print("æœªçˆ¬å–åˆ°ä»»ä½•å†…å®¹ã€‚")
